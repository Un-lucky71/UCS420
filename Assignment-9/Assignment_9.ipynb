{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.** Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports, technology, food, books, etc.).\n",
        "\n",
        "1) Convert text to lowercase and remove punctuaƟon.\n",
        "\n",
        "2) Tokenize the text into words and sentences.\n",
        "\n",
        "3) Remove stopwords (using NLTK's stopwords list).\n",
        "\n",
        "4) Display word frequency distribuition (excluding stopwords)."
      ],
      "metadata": {
        "id": "mYKOmS4og1PV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1kbi9Iqf1al",
        "outputId": "1c266197-2284-45df-f39c-57d3c0e747f8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "\n",
        "text = \"\"\"Books are gateways to new worlds, allowing us to explore places, people, and ideas through the written word. They educate, inspire, and sometimes transport us to realms of fantasy and adventure. Fiction cultivates creativity, while non-fiction provides knowledge and perspective. The smell of a freshly opened book has an inexplicable charm, and digital books offer convenience on-the-go. Reading is not just a hobby; it's a lifelong journey into wisdom and imagination.\"\"\"\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "text = text.lower()\n",
        "text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "\n",
        "word_tokens = word_tokenize(text)\n",
        "sentence_tokens = sent_tokenize(text)\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in word_tokens if word not in stop_words]\n",
        "\n",
        "\n",
        "word_freq = Counter(filtered_words)\n",
        "\n",
        "print(\"Sentence Tokens:\", sentence_tokens)\n",
        "print(\"Filtered Words:\", filtered_words)\n",
        "print(\"Word Frequency Distribution:\", word_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTZP824XgaJx",
        "outputId": "29128a93-b4f6-41b3-aaa9-c3444c0ee683"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokens: ['books are gateways to new worlds allowing us to explore places people and ideas through the written word they educate inspire and sometimes transport us to realms of fantasy and adventure fiction cultivates creativity while nonfiction provides knowledge and perspective the smell of a freshly opened book has an inexplicable charm and digital books offer convenience onthego reading is not just a hobby its a lifelong journey into wisdom and imagination']\n",
            "Filtered Words: ['books', 'gateways', 'new', 'worlds', 'allowing', 'us', 'explore', 'places', 'people', 'ideas', 'written', 'word', 'educate', 'inspire', 'sometimes', 'transport', 'us', 'realms', 'fantasy', 'adventure', 'fiction', 'cultivates', 'creativity', 'nonfiction', 'provides', 'knowledge', 'perspective', 'smell', 'freshly', 'opened', 'book', 'inexplicable', 'charm', 'digital', 'books', 'offer', 'convenience', 'onthego', 'reading', 'hobby', 'lifelong', 'journey', 'wisdom', 'imagination']\n",
            "Word Frequency Distribution: Counter({'books': 2, 'us': 2, 'gateways': 1, 'new': 1, 'worlds': 1, 'allowing': 1, 'explore': 1, 'places': 1, 'people': 1, 'ideas': 1, 'written': 1, 'word': 1, 'educate': 1, 'inspire': 1, 'sometimes': 1, 'transport': 1, 'realms': 1, 'fantasy': 1, 'adventure': 1, 'fiction': 1, 'cultivates': 1, 'creativity': 1, 'nonfiction': 1, 'provides': 1, 'knowledge': 1, 'perspective': 1, 'smell': 1, 'freshly': 1, 'opened': 1, 'book': 1, 'inexplicable': 1, 'charm': 1, 'digital': 1, 'offer': 1, 'convenience': 1, 'onthego': 1, 'reading': 1, 'hobby': 1, 'lifelong': 1, 'journey': 1, 'wisdom': 1, 'imagination': 1})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2.** Stemming and LemmaƟzaƟon\n",
        "\n",
        "1) Take the tokenized words from QuesƟon 1 (aŌer stopword removal).\n",
        "\n",
        "2) Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
        "\n",
        "3) Apply lemmaƟzaƟon using NLTK's WordNetLemmaƟzer.\n",
        "\n",
        "4) Compare and display results of both techniques.\n"
      ],
      "metadata": {
        "id": "Nw8V4gLYhBlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "stemmed_porter = [porter.stem(word) for word in filtered_words]\n",
        "stemmed_lancaster = [lancaster.stem(word) for word in filtered_words]\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "print(\"Porter Stemmer:\", stemmed_porter)\n",
        "print(\"Lancaster Stemmer:\", stemmed_lancaster)\n",
        "print(\"Lemmatized Words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_uqOL3thK-t",
        "outputId": "c1e8d2bb-a39e-4a78-9ea8-755c71c4bdb2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer: ['book', 'gateway', 'new', 'world', 'allow', 'us', 'explor', 'place', 'peopl', 'idea', 'written', 'word', 'educ', 'inspir', 'sometim', 'transport', 'us', 'realm', 'fantasi', 'adventur', 'fiction', 'cultiv', 'creativ', 'nonfict', 'provid', 'knowledg', 'perspect', 'smell', 'freshli', 'open', 'book', 'inexplic', 'charm', 'digit', 'book', 'offer', 'conveni', 'onthego', 'read', 'hobbi', 'lifelong', 'journey', 'wisdom', 'imagin']\n",
            "Lancaster Stemmer: ['book', 'gateway', 'new', 'world', 'allow', 'us', 'expl', 'plac', 'peopl', 'idea', 'writ', 'word', 'educ', 'inspir', 'sometim', 'transport', 'us', 'realm', 'fantasy', 'adv', 'fict', 'cult', 'cre', 'nonfict', 'provid', 'knowledg', 'perspect', 'smel', 'fresh', 'op', 'book', 'inexpl', 'charm', 'digit', 'book', 'off', 'conveny', 'onthego', 'read', 'hobby', 'lifelong', 'journey', 'wisdom', 'imagin']\n",
            "Lemmatized Words: ['book', 'gateway', 'new', 'world', 'allowing', 'u', 'explore', 'place', 'people', 'idea', 'written', 'word', 'educate', 'inspire', 'sometimes', 'transport', 'u', 'realm', 'fantasy', 'adventure', 'fiction', 'cultivates', 'creativity', 'nonfiction', 'provides', 'knowledge', 'perspective', 'smell', 'freshly', 'opened', 'book', 'inexplicable', 'charm', 'digital', 'book', 'offer', 'convenience', 'onthego', 'reading', 'hobby', 'lifelong', 'journey', 'wisdom', 'imagination']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3.** Regular Expressions and Text Spliƫng\n",
        "\n",
        "1) Take their original text from QuesƟon 1.\n",
        "\n",
        "2) Use regular expressions to: a. Extract all words with more than 5 leƩers. b. Extract all numbers (if any exist in their text). c. Extract all capitalized words.\n",
        "\n",
        "3) Use text spliƫng techniques to: a. Split the text into words containing only alphabets (removing digits and special characters). b. Extract words starƟng with a vowel."
      ],
      "metadata": {
        "id": "4o8ggMmghPpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "\n",
        "\n",
        "long_words = re.findall(r'\\b\\w{6,}\\b', text)\n",
        "\n",
        "\n",
        "numbers = re.findall(r'\\d+', text)\n",
        "\n",
        "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', \"Technology is reshaping the world.\")\n",
        "\n",
        "print(\"Words with >5 letters:\", long_words)\n",
        "print(\"Numbers:\", numbers)\n",
        "print(\"Capitalized Words:\", capitalized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OtfjP9JhX3n",
        "outputId": "2da27db0-66e2-48d3-c8bd-fb0936725996"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words with >5 letters: ['gateways', 'worlds', 'allowing', 'explore', 'places', 'people', 'through', 'written', 'educate', 'inspire', 'sometimes', 'transport', 'realms', 'fantasy', 'adventure', 'fiction', 'cultivates', 'creativity', 'nonfiction', 'provides', 'knowledge', 'perspective', 'freshly', 'opened', 'inexplicable', 'digital', 'convenience', 'onthego', 'reading', 'lifelong', 'journey', 'wisdom', 'imagination']\n",
            "Numbers: []\n",
            "Capitalized Words: ['Technology']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alphabet_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
        "vowel_words = [word for word in alphabet_words if word[0] in 'aeiou']\n",
        "print(\"Alphabet words:\", alphabet_words)\n",
        "print(\"Vowel-starting words:\", vowel_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v26dXiAfhcoc",
        "outputId": "e5da35e4-4cca-4d48-9652-b3ea348bd1cd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alphabet words: ['books', 'are', 'gateways', 'to', 'new', 'worlds', 'allowing', 'us', 'to', 'explore', 'places', 'people', 'and', 'ideas', 'through', 'the', 'written', 'word', 'they', 'educate', 'inspire', 'and', 'sometimes', 'transport', 'us', 'to', 'realms', 'of', 'fantasy', 'and', 'adventure', 'fiction', 'cultivates', 'creativity', 'while', 'nonfiction', 'provides', 'knowledge', 'and', 'perspective', 'the', 'smell', 'of', 'a', 'freshly', 'opened', 'book', 'has', 'an', 'inexplicable', 'charm', 'and', 'digital', 'books', 'offer', 'convenience', 'onthego', 'reading', 'is', 'not', 'just', 'a', 'hobby', 'its', 'a', 'lifelong', 'journey', 'into', 'wisdom', 'and', 'imagination']\n",
            "Vowel-starting words: ['are', 'allowing', 'us', 'explore', 'and', 'ideas', 'educate', 'inspire', 'and', 'us', 'of', 'and', 'adventure', 'and', 'of', 'a', 'opened', 'an', 'inexplicable', 'and', 'offer', 'onthego', 'is', 'a', 'its', 'a', 'into', 'and', 'imagination']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4.** Custom TokenizaƟon & Regex-based Text Cleaning\n",
        "\n",
        "1) Take original text from Question 1.\n",
        "\n",
        "2) Write a custom tokenization function that: a. Removes punctuation and special symbols, but keeps contractions (e.g., \"isn't\" should not be split into \"is\" and \"n't\"). b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains a single token). c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\" should remain as is).\n",
        "\n",
        "3) Use Regex SubsƟtuƟons (re.sub) to: a. Replace email addresses with '' placeholder. b. Replace URLs with '' placeholder. c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with '' placeholder."
      ],
      "metadata": {
        "id": "-TSIM3JhhgAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_tokenize(text):\n",
        "\n",
        "    tokens = re.findall(r'\\b\\w[\\w-]*\\b', text)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "text = \"Contact me at abc@example.com or visit https://example.com. Call +91 9876543210.\"\n",
        "cleaned_text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
        "cleaned_text = re.sub(r'(https?:\\/\\/\\S+|www\\.\\S+)', '', cleaned_text)\n",
        "cleaned_text = re.sub(r'\\+?\\d[\\d -]{8,}\\d', '', cleaned_text)\n",
        "\n",
        "print(\"Custom Tokens:\", custom_tokenize(text))\n",
        "print(\"Cleaned Text:\", cleaned_text)"
      ],
      "metadata": {
        "id": "6t_Xz3U6h4CF",
        "outputId": "5e03511b-162b-46f6-de5c-fd2c85bc5370",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Tokens: ['Contact', 'me', 'at', 'abc', 'example', 'com', 'or', 'visit', 'https', 'example', 'com', 'Call', '91', '9876543210']\n",
            "Cleaned Text: Contact me at  or visit  Call .\n"
          ]
        }
      ]
    }
  ]
}